#!/usr/bin/env python3
"""
GLM Assistant MCP Server
í† í° ì ˆì•½ì„ ìœ„í•œ GLM-4.5-Air í†µí•© ì„œë²„
"""

import os
import json
import asyncio
from typing import Any
import httpx

# MCP SDK ì„í¬íŠ¸
try:
    from mcp.server import Server
    from mcp.types import Tool, TextContent
    import mcp.server.stdio
except ImportError:
    print("MCP SDK not installed. Run: pip install mcp", file=__import__('sys').stderr)
    exit(1)

# í™˜ê²½ ì„¤ì •
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1"
GLM_MODEL = "zai-org/glm-4.5-air:free"

# MCP ì„œë²„ ì´ˆê¸°í™”
app = Server("glm-assistant")


async def call_glm(prompt: str, system_prompt: str = None, max_tokens: int = 4000) -> str:
    """GLM API í˜¸ì¶œ"""
    if not OPENROUTER_API_KEY:
        return "âŒ OPENROUTER_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})

    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
            f"{OPENROUTER_BASE_URL}/chat/completions",
            headers={
                "Authorization": f"Bearer {OPENROUTER_API_KEY}",
                "HTTP-Referer": "https://claude-code.local",
                "X-Title": "Claude Code GLM Assistant"
            },
            json={
                "model": GLM_MODEL,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": 0.7
            }
            )
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["message"]["content"]
    except Exception as e:
        return f"âŒ GLM API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"


@app.list_tools()
async def list_tools() -> list[Tool]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡"""
    return [
        Tool(
            name="ask_glm",
            description="í† í° ì ˆì•½ìš© GLM-4.5-Airì—ê²Œ ì¼ë°˜ ì§ˆë¬¸í•˜ê¸°. ê°„ë‹¨í•œ ì‘ì—…, ë²ˆì—­, ìš”ì•½, ì„¤ëª… ë“±ì— í™œìš©.",
            inputSchema={
                "type": "object",
                "properties": {
                    "question": {
                        "type": "string",
                        "description": "GLMì—ê²Œ í•  ì§ˆë¬¸"
                    }
                },
                "required": ["question"]
            }
        ),
        Tool(
            name="ask_glm_code",
            description="GLMì—ê²Œ ì½”ë“œ ê´€ë ¨ ì§ˆë¬¸í•˜ê¸°. ì½”ë“œ ë¦¬ë·°, ë²„ê·¸ ë¶„ì„, ê°„ë‹¨í•œ ë¦¬íŒ©í† ë§ ì œì•ˆ ë“±ì— í™œìš©.",
            inputSchema={
                "type": "object",
                "properties": {
                    "code": {
                        "type": "string",
                        "description": "ë¶„ì„í•  ì½”ë“œ"
                    },
                    "question": {
                        "type": "string",
                        "description": "ì½”ë“œì— ëŒ€í•œ ì§ˆë¬¸"
                    }
                },
                "required": ["code", "question"]
            }
        ),
        Tool(
            name="ask_glm_quick",
            description="GLMì—ê²Œ ë¹ ë¥¸ ë‹µë³€ ìš”ì²­. ì§§ì€ ì§ˆë¬¸, Yes/No, ê°„ë‹¨í•œ í™•ì¸ ë“±ì— í™œìš©.",
            inputSchema={
                "type": "object",
                "properties": {
                    "question": {
                        "type": "string",
                        "description": "ë¹ ë¥´ê²Œ ë‹µë³€ë°›ì„ ì§ˆë¬¸"
                    }
                },
                "required": ["question"]
            }
        )
    ]


@app.call_tool()
async def call_tool(name: str, arguments: Any) -> list[TextContent]:
    """ë„êµ¬ ì‹¤í–‰"""

    if name == "ask_glm":
        question = arguments.get("question", "")
        answer = await call_glm(question)
        return [TextContent(type="text", text=f"ğŸ¤– GLM: {answer}")]

    elif name == "ask_glm_code":
        code = arguments.get("code", "")
        question = arguments.get("question", "")

        system_prompt = """ë‹¹ì‹ ì€ ì½”ë“œ ë¦¬ë·°ì™€ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì½”ë“œì˜ ë¬¸ì œì , ê°œì„  ë°©ì•ˆ, ë²„ê·¸ë¥¼ ì •í™•í•˜ê²Œ íŒŒì•…í•˜ê³  ê°„ê²°í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤."""

        prompt = f"""ë‹¤ìŒ ì½”ë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

```
{code}
```

ì§ˆë¬¸: {question}"""

        answer = await call_glm(prompt, system_prompt=system_prompt)
        return [TextContent(type="text", text=f"ğŸ¤– GLM (ì½”ë“œ ë¶„ì„): {answer}")]

    elif name == "ask_glm_quick":
        question = arguments.get("question", "")
        system_prompt = "ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ í•µì‹¬ë§Œ ë‹µë³€í•˜ì„¸ìš”. 2-3ë¬¸ì¥ ì´ë‚´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤."
        answer = await call_glm(question, system_prompt=system_prompt, max_tokens=500)
        return [TextContent(type="text", text=f"ğŸ¤– GLM (ë¹ ë¥¸ ë‹µë³€): {answer}")]

    else:
        return [TextContent(type="text", text=f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {name}")]


async def main():
    """MCP ì„œë²„ ì‹¤í–‰"""
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await app.run(
            read_stream,
            write_stream,
            app.create_initialization_options()
        )


if __name__ == "__main__":
    asyncio.run(main())
